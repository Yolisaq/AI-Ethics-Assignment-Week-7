# ğŸ¤– AI Fairness Audit: COMPAS Recidivism Dataset âš–ï¸

## ğŸŒŸ Project Overview
This project demonstrates a **practical audit of AI fairness** using the COMPAS Recidivism Dataset.  
We analyze **racial bias** in risk prediction scores, apply **bias mitigation techniques**, and evaluate model performance and fairness metrics.  

**Key Goals:**
- Detect and quantify algorithmic bias ğŸŒˆ
- Apply fairness-aware algorithms ğŸ› ï¸
- Visualize disparities and evaluate model accuracy ğŸ“Š
- Promote ethical AI design and responsible decision-making ğŸ§ 

---

## ğŸ› ï¸ Tools & Libraries
- **Python 3.13** ğŸ  
- **Pandas** for data manipulation ğŸ“‘  
- **Matplotlib & Seaborn** for visualizations ğŸ“Š  
- **AI Fairness 360 (AIF360)** for bias detection and mitigation âš–ï¸  
- **Scikit-learn** for logistic regression and evaluation ğŸ§®  

---

## ğŸ“‚ Dataset
**COMPAS Recidivism Dataset**  
- Contains risk scores for criminal recidivism ğŸ•µï¸â€â™‚ï¸  
- Features include age, sex, race, prior offenses, and risk score ğŸ“  
- Goal: Identify and mitigate racial bias in risk prediction  

---

## ğŸš€ How to Run
1. Clone the repository:  
```bash
git clone [<your-repo-url>](https://github.com/Yolisaq/AI-Ethics-Assignment-Week-7.git)
